{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EugenioBugli/PointCloud3D/blob/main/PointCloud3D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_0FERiAhMW8"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kBbL55fBHVY2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q open3d\n",
        "!pip install -q torch_scatter\n",
        "!pip install ninja\n",
        "!pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\n",
        "!pip install trimesh\n",
        "!pip install trimesh[easy]\n",
        "# !pip install pyfqmr\n",
        "# !pip install .\n",
        "!git clone https://github.com/EugenioBugli/PointCloud3D.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su0uRyWQE_On",
        "outputId": "b5728a17-cece-4eed-eb21-2ec956ccf020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using PyTorch version: 2.5.1+cu121  Device: cuda\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import tqdm\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_scatter import scatter_mean, scatter_max\n",
        "\n",
        "import open3d as o3d\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from pytorch3d.loss import chamfer_distance\n",
        "from scipy.spatial import cKDTree\n",
        "import trimesh\n",
        "# import pyfqmr\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'/content/PointCloud3D')\n",
        "from PointCloud3D.src.dataset import FAUST_Dataset, LoadDataset, openDataFiles\n",
        "from PointCloud3D.src.utils import Plot2D, voxel2Numpy, PlotVoxel, Plot2DWithBuckets, PlotOccupancyMesh, o3d2Trimesh, toPointCloud, Cloud2Voxel\n",
        "from PointCloud3D.src.unet import UNet\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using PyTorch version:', torch.__version__, ' Device:', device)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "drive_path = \"/content/drive/MyDrive/CV/MPI-FAUST\"\n",
        "training_path = drive_path + \"/training\"\n",
        "test_path = drive_path + \"/test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pJUMCQszcoPr"
      },
      "outputs": [],
      "source": [
        "SAMPLING_TYPE = \"RANDOM\"\n",
        "SAMPLING_SIZE = 2048\n",
        "BATCH_SIZE = 5\n",
        "\n",
        "IN_DIM_RES_PT = 64\n",
        "FEATURES_DIM = 32\n",
        "NUM_BLOCKS = 5\n",
        "NUM_FC = 4\n",
        "NUM_PLANES = 4\n",
        "NUM_COORDINATES = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFBi0FgT9WKe"
      },
      "source": [
        "# 1] Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2UANsdbf_on3"
      },
      "outputs": [],
      "source": [
        "# use this to create the dataset\n",
        "\n",
        "# train_scan_files, train_reg_files, val_scan_files, val_reg_files, test_scan_files, test_reg_files = openDataFiles(training_path, test_path, val_size=0.2)\n",
        "# train_set = FAUST_Dataset(train_scan_files, train_reg_files, sampling_size=SAMPLING_SIZE, partition=\"TRAIN\", transform=transforms.ToTensor())\n",
        "# val_set = FAUST_Dataset(val_scan_files, val_reg_files, sampling_size=SAMPLING_SIZE, partition=\"VAL\", transform=transforms.ToTensor())\n",
        "# test_set = FAUST_Dataset(test_scan_files, test_reg_files, sampling_size=SAMPLING_SIZE, partition=\"TEST\", transform=transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6oNsI_i6ydBj"
      },
      "outputs": [],
      "source": [
        "train_set = LoadDataset(\"train_set.pt\")\n",
        "val_set = LoadDataset(\"val_set.pt\")\n",
        "test_set = LoadDataset(\"test_set.pt\")\n",
        "\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_set, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC0Pc51bA4sO"
      },
      "source": [
        "# 1.2] Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AEZANsN497B",
        "outputId": "81ff2bb0-298a-4616-a141-5ab9e09e0653"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([5, 3000, 3]) torch.Size([5, 6890, 3]) torch.Size([5, 2048, 3])\n",
            "5 5\n"
          ]
        }
      ],
      "source": [
        "for batch in train_loader:\n",
        "    sampled_scan = batch[0]\n",
        "    registration = batch[1]\n",
        "    query = batch[2]\n",
        "    scan_path = batch[3]\n",
        "    reg_path = batch[4]\n",
        "    print(sampled_scan.shape, registration.shape, query.shape)\n",
        "    print(len(scan_path), len(reg_path))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh8ugKUp9WKf"
      },
      "source": [
        "# 2] Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8abAwINI9WKf"
      },
      "source": [
        "## 2.1) ResBlock + Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OZfg7nQw9WKf"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    \"\"\"\n",
        "        This class is used to define a Residual Block, which is one of the main component of the ResNetPointNet architecture\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim=64, n_points=2048, h_dim=32, out_dim=64):\n",
        "        super(ResBlock, self).__init__()\n",
        "\n",
        "        self.n_points = n_points\n",
        "\n",
        "        #> First part of the Block\n",
        "\n",
        "        self.fc1 = nn.Linear(\n",
        "            in_dim,\n",
        "            h_dim\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm1d(self.n_points)\n",
        "\n",
        "        #> Second part of the Block\n",
        "\n",
        "        self.fc2 = nn.Linear(\n",
        "            h_dim,\n",
        "            out_dim\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm1d(self.n_points)\n",
        "\n",
        "        #> Skip connection\n",
        "\n",
        "        if in_dim != out_dim:\n",
        "            # size mismatch (never happen in my case)\n",
        "            self.residual = nn.Linear(in_dim, out_dim)\n",
        "        else:\n",
        "            # same size\n",
        "            self.residual = None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: (b,p,in_dim) = (b,p,64)\n",
        "\n",
        "        first_part = F.relu(self.bn1(self.fc1(x))) # (b,p,64) -> (b,p,32)\n",
        "\n",
        "        second_part = self.bn2(self.fc2(first_part)) # (b,p,32) -> (b,p,64)\n",
        "\n",
        "        if self.residual is None:\n",
        "            # no size mismatch\n",
        "            res = x # (b,p,64)\n",
        "        else:\n",
        "            # transformation if there is a size mismatch in_dim != out_dim\n",
        "            res = self.residual(x) # (b,p,in_dim) -> (b,p,64)\n",
        "\n",
        "        # add residual connection\n",
        "        third_part = second_part + res # (b,p,64) -> (b,p,64)\n",
        "\n",
        "        return F.relu(third_part)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk_D19aI9WKg"
      },
      "source": [
        "## 2.2) ResNetPointNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v_Do5AElGE7k"
      },
      "outputs": [],
      "source": [
        "def CloudNormalization(input_cloud):\n",
        "    # normalize cloud in order to have it in [0,1]\n",
        "    min = input_cloud.min()\n",
        "    max = input_cloud.max()\n",
        "    norm_cloud = (input_cloud - min)/(max-min)\n",
        "    return norm_cloud\n",
        "\n",
        "def CanonicalProjection(input_cloud):\n",
        "    # gives you a dict with all 3 canonical projections (xy, xz, yz)\n",
        "    xy_proj = CloudNormalization(input_cloud[:,:,[0,1]]) # XY plane (view from above)\n",
        "    xz_proj = CloudNormalization(input_cloud[:,:,[0,2]]) # XZ plane (view from long side)\n",
        "    yz_proj = CloudNormalization(input_cloud[:,:,[1,2]]) # YZ plane (view from short side)\n",
        "    return {\"xy\": xy_proj, \"xz\": xz_proj, \"yz\": yz_proj}\n",
        "\n",
        "def DivideInBuckets(input_cloud_proj, resolution):\n",
        "    # in input we have the canonical projection of the input cloud and the resolution of our buckets\n",
        "    # map 2D coordinates into one dimension\n",
        "    xy_buck = (input_cloud_proj[\"xy\"]*resolution).long()\n",
        "    xz_buck = (input_cloud_proj[\"xz\"]*resolution).long()\n",
        "    yz_buck = (input_cloud_proj[\"yz\"]*resolution).long()\n",
        "    return {\"xy\": xy_buck[:,:,0] + resolution*xy_buck[:,:,1], \"xz\": xz_buck[:,:,0] + resolution*xz_buck[:,:,1], \"yz\": yz_buck[:,:,0] + resolution*yz_buck[:,:,1]}\n",
        "\n",
        "def LocalMaxPooling(input_features, input_cloud, resolution):\n",
        "    # given the 3D point cloud I need to project them into a 2D grid. In this way I can define the neighborhood to perform local pooling\n",
        "    # voxelization is not a good idea since I will lose details (all points are averaged into a voxel)\n",
        "    canon_proj = CanonicalProjection(input_cloud)\n",
        "    # we need to divide this 2D planes into buckets and the perform inside each of them the pooling operation\n",
        "    indices = DivideInBuckets(canon_proj, resolution) # only the full buckets are taken from the resolution^2 buckets\n",
        "    pooled_dict = dict()\n",
        "    gathered_container = 0\n",
        "    for plane in canon_proj.keys():\n",
        "        projection_indices = indices[plane]\n",
        "        scatter_container = []\n",
        "        # max pooling operation over the input_features (batch, num_points, 32)\n",
        "        for batch in range(input_cloud.shape[0]):\n",
        "            features = input_features[batch]\n",
        "            per_batch_indices = projection_indices[batch]\n",
        "\n",
        "            scatter_batch, _ = scatter_max(features, per_batch_indices, dim=0, dim_size=resolution**2)\n",
        "            scatter_container.append(scatter_batch)\n",
        "\n",
        "        scatter_container = torch.stack(scatter_container, dim=0) # (batch_size, resolution**2, features_dim)\n",
        "        pooled_dict[plane] = scatter_container\n",
        "        # perform the gathering operation over the max point of each plane\n",
        "        projection_indices_clamp = projection_indices.clamp_(0, resolution**2-1)\n",
        "        gathered = scatter_container.gather(dim=1, index=projection_indices_clamp.unsqueeze(-1).expand(input_cloud.shape[0], input_cloud.shape[1], scatter_container.size(-1)))\n",
        "        gathered_container += gathered\n",
        "    return gathered_container # (batch_size, num_points, features_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qYhVqWYM9WKg"
      },
      "outputs": [],
      "source": [
        "class ResNetPointNet(nn.Module):\n",
        "    \"\"\"\n",
        "        This class is used to define the PointNet model used to form a feature embedding for each point in the Point Cloud given in input.\n",
        "        Architecture design:\n",
        "\n",
        "            @ INPUT: Tensor of shape (batch_size, num_points, 3)\n",
        "\n",
        "            > Fully Connected Layer (3, in_dim=64)\n",
        "            > 5 Residual Blocks with Local Pooling and Concatenation\n",
        "            > Fully Connected Layer (out_dim=32, out_dim=32)\n",
        "\n",
        "            @ OUTPUT: Tensor of shape (batch_size, num_points, 32)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim=64, n_points=2048, res_dim=32, out_dim=32, n_blocks=5):\n",
        "        super(ResNetPointNet, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(3, in_dim)\n",
        "\n",
        "        self.res = nn.ModuleList([\n",
        "            ResBlock(in_dim, n_points, res_dim, out_dim) for n_res in range(n_blocks)\n",
        "        ])\n",
        "\n",
        "        self.fc2 = nn.Linear(out_dim, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: (b,p,3)\n",
        "\n",
        "        # Extract Normalized coordinates and indices to perform local pooling\n",
        "\n",
        "        norm_coord = CanonicalProjection(x) # (b,p,2)\n",
        "        coord_indices = DivideInBuckets(norm_coord, 32) # (b,p,2)\n",
        "\n",
        "        # First FC Layer\n",
        "        fc1 = F.relu(self.fc1(x)) # (b,p,3) -> (b,p,64)\n",
        "        #print(\"fc1: \",fc1.shape)\n",
        "        # First ResBlock\n",
        "        res = self.res[0](fc1) # (b,p,64) -> (b,p,32)\n",
        "        #print(\"res: \",res.shape)\n",
        "        # 2-5 ResBlock\n",
        "        for res_block in self.res[1:]:\n",
        "            pool = LocalMaxPooling(res, x, 32) # (b,p,32)\n",
        "            #print(\"pool: \",pool.shape)\n",
        "            concat = torch.cat([res, pool], dim=2) # (b,p,32) | (b,p,32) -> (b,p,64)\n",
        "            #print(\"concatenation: \",concat.shape)\n",
        "            # following residual block\n",
        "            res = res_block(concat) # (b,p,64) -> (b,p,32)\n",
        "\n",
        "        # Last FC Layer\n",
        "        final = F.relu(self.fc2(res)) # (b,p,32) -> (b,p,32)\n",
        "\n",
        "        #   print(f\"ResNetPointNet : input ({x.shape}) ---> output ({final.shape}) \\n\")\n",
        "\n",
        "        return final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHoALeYs9WKg"
      },
      "source": [
        "## 2.3) Plane Predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ASem9l-E9WKg"
      },
      "outputs": [],
      "source": [
        "class SimplePointNet(nn.Module):\n",
        "    \"\"\"\n",
        "        This class is used to define a simple variant of the PointNet Model, which is one of the main components of the Plane Predictor Network.\n",
        "        This Network will provide us a global context of the Input Point Clouds\n",
        "        Architecture Design:\n",
        "\n",
        "            @ INPUT: Tensor of shape (batch_size, num_points, 3) which represent the Input Point Clouds\n",
        "\n",
        "            > Fully Connected Layer (3, 64)\n",
        "\n",
        "              |> Fully Connected Layer (64, 32)\n",
        "            2*|> Global Max Pooling\n",
        "              |> Concatenation btw Pooled and unpooled features\n",
        "\n",
        "            > Fully Connected Layer (64, 32)\n",
        "            > Global Max Pooling\n",
        "\n",
        "            @ OUTPUT: Tensor of shape (batch_size, 1, 32) which will be used by the rest of the Plane Predictor\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, in_dim=64, n_points=2048, hid_dim=32, out_dim=64):\n",
        "        super(SimplePointNet, self).__init__()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.n_points = n_points\n",
        "\n",
        "        self.initial_fc = nn.Linear(in_features=3, out_features=in_dim)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=in_dim, out_features=hid_dim)\n",
        "\n",
        "        self.fc2 = nn.Linear(in_features=in_dim, out_features=hid_dim)\n",
        "\n",
        "        self.final_fc = nn.Linear(in_features=in_dim, out_features=hid_dim)\n",
        "\n",
        "        self.pool = nn.AdaptiveMaxPool1d(output_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.initial_fc(x)) # (b,p,3) --> (b,p,64)\n",
        "\n",
        "        x1 = F.relu(self.fc1(x)) # (b,p,64) --> (b,p,32)\n",
        "        x1_t = x1.transpose(1,2) # (b,p,32) -> (b,32,p)\n",
        "        pool_x1 = self.pool(x1_t) # (b,32,p) -> (b,32,1)\n",
        "        exp_pool_x1 = pool_x1.transpose(1,2).expand(self.batch_size, self.n_points, 32) # (b,32,1) -> (b,1,32) -> (b,p,32)\n",
        "        concat_x1 = torch.cat([x1, exp_pool_x1], dim=2) # (b,p,32) | (b,p,32) -> (b,p,64)\n",
        "\n",
        "        x2 = F.relu(self.fc2(concat_x1)) # (b,p,64) -> (b,p,32)\n",
        "        x2_t = x2.transpose(1,2) # (b,p,32) -> (b,32,p)\n",
        "        pool_x2 = self.pool(x2_t) # (b,32,p) -> (b,32,1)\n",
        "        exp_pool_x2 = pool_x2.transpose(1,2).expand(self.batch_size, self.n_points, 32) # (b,32,1) -> (b,1,32) -> (b,p,32)\n",
        "        concat_x2 = torch.cat([x2, exp_pool_x2], dim=2) # (b,p,32) | (b,p,32) -> (b,p,64)\n",
        "\n",
        "        pre_pool_out = F.relu(self.final_fc(concat_x2)) # (b,p,64) -> (b,p,32)\n",
        "        pre_pool_out_t = pre_pool_out.transpose(1,2) # (b,p,32) -> (b,32,p)\n",
        "        out = self.pool(pre_pool_out_t) # (b,32,p) -> (b,32,1)\n",
        "\n",
        "        # print(f\"SimplePointNet : input ({x.shape}) ---> output ({out.transpose(1,2).shape}) \\n\")\n",
        "        return out.transpose(1,2) # (b,32,1) -> (b,1,32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6cWO2Alj9WKg"
      },
      "outputs": [],
      "source": [
        "class PlanePredictor(nn.Module):\n",
        "    \"\"\"\n",
        "        This class is used to define the Plane Predictor of our Architecture, which will predict the plane parameters of L dynamic planes\n",
        "        Architecture design:\n",
        "\n",
        "            @ INPUT: Tensor of shape (batch_size, num_points, 3) which represent Point Clouds\n",
        "\n",
        "            > Simple PointNet which learns the global context of the input point clouds\n",
        "            > This information is encoded into one global feature by using Max Pooling\n",
        "            > 4 Fully Connected Layers with hidden dimension = 32\n",
        "            > Transform this features with a shallow net (from 32 to L dim) (plane parameters)\n",
        "            > Refine each plane parameter with a Shallow net\n",
        "            > Get plane features (from L to 32 dim)\n",
        "            > Each plane-specific feature is expanded to N x D to match the output of the point cloud encoder, which will be summed together\n",
        "\n",
        "\n",
        "            @ OUTPUT: Tensor of shape (batch_size, num_points, 32) which will be summed up to the features given by the ResNetPointNet before\n",
        "                      being processed into U-Net and the plane parameter tensor (batch_size, L, 3)\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=32, n_points=2048, n_fc=4, L=4):\n",
        "        super(PlanePredictor, self).__init__()\n",
        "\n",
        "        self.pointNet = SimplePointNet(batch_size=BATCH_SIZE, in_dim=IN_DIM_RES_PT, n_points=n_points, hid_dim=in_dim, out_dim=IN_DIM_RES_PT)\n",
        "        self.n_points = n_points\n",
        "        self.L = L\n",
        "\n",
        "        self.four_fc = nn.ModuleList(\n",
        "            [nn.Linear(in_dim, in_dim) for i in range(n_fc)]\n",
        "        )\n",
        "        # want L plane parameters\n",
        "        self.feat_to_plane = nn.Linear(in_dim, 3*self.L)\n",
        "\n",
        "        self.L_shallows = nn.ModuleList(\n",
        "            [nn.Linear(3, 3) for i in range(self.L)]\n",
        "        )\n",
        "\n",
        "        # L FC layers with hidden dim = 32\n",
        "\n",
        "        self.L_plane_to_features = nn.ModuleList(\n",
        "            [nn.Linear(3, in_dim) for i in range(self.L)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        flc = self.pointNet(x) # (b,p,3) -> (b,1,32) # features from simplepointnet\n",
        "\n",
        "        for fc in self.four_fc: # 4 fully connected block\n",
        "            flc = F.relu(fc(flc))  # (b,1,32) -> (b,1,32)\n",
        "\n",
        "        plane_param = F.relu(self.feat_to_plane(flc)) # (b,1,32) -> (b,1,3*L)\n",
        "        plane_param = plane_param.view(plane_param.shape[0], self.L, 3) # (b,1,3*L) -> (b,L,3)\n",
        "\n",
        "        stack_of_planes = []\n",
        "        features = 0\n",
        "        for p in range(self.L):\n",
        "            # I need to refine the plane parameters by passing them into a separate shallow net\n",
        "            pth_plane = F.relu(self.L_shallows[p](plane_param[:,p,:])) # (b,3) -> (b,3)\n",
        "            stack_of_planes.append(pth_plane)\n",
        "\n",
        "            pth_feat = F.relu(self.L_plane_to_features[p](pth_plane)) # (b,3) -> (b,32)\n",
        "            features += pth_feat.unsqueeze(1)\n",
        "\n",
        "        out_features = features.expand(x.shape[0], self.n_points, pth_feat.shape[-1]) # (b,L,32) -> (b,p,32)\n",
        "        stack_of_planes = torch.stack(stack_of_planes, dim=1)\n",
        "        # print(f\"PlanePredictor : input ({x.shape}) ---> output ({out_features.shape}, {stack_of_planes.shape}) \\n\")\n",
        "        return out_features, stack_of_planes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XZAjemIsD0r"
      },
      "source": [
        "## 2.4) Feature Projection Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erplhNg6SWjx"
      },
      "source": [
        "$R = I + skew(v) + skew(v)^2 \\cdot \\frac{1-\\mathcal{k} \\cdot \\mathcal{\\hat{n}}} {||v||^2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RFl41LogP_UD"
      },
      "outputs": [],
      "source": [
        "# we need to project the encoded features (the output of the previous step aka 2.2+2.3) onto the dynamic planes with a defined size of H x W grid and then\n",
        "# apply max-pooling for the features inside each cell\n",
        "# 3 different operations must be employed to keep them inside H x W grids.\n",
        "# > Basis Change\n",
        "# > Orthographic Projection aka project the summed-up features onto the dynamic planes\n",
        "# > Normalization\n",
        "# > apply local max pooling to each bucket (H*W buckets)\n",
        "\n",
        "def skew_symm(v):\n",
        "    if len(v.shape) == 2: return torch.tensor([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]]).to(device)\n",
        "    else: # if you pass a batch of vectors\n",
        "        sk = torch.zeros((v.shape[0], 3, 3)).to(device)\n",
        "        for j in range(v.shape[0]):\n",
        "            sk[j] = skew_symm(v[j])\n",
        "        return sk\n",
        "\n",
        "def dot_prod(v1, v2): # assume that you always pass 2 vect with same dimension (2D or 3D)\n",
        "    if len(v1.shape) == 2:\n",
        "        return torch.bmm(v1.unsqueeze(2), v2.unsqueeze(1))\n",
        "    return torch.bmm(v1.transpose(1,2),v2)\n",
        "\n",
        "\n",
        "# Given the vectors, of the 3 canonical axes, (i,j,k) and the learned plane normal n we need to perform a change of basis:\n",
        "# First we transform n into a unit vector n_hat\n",
        "# We compute the rotation matrix that aligns k (basic vector of the ground plane) and n_hat by following the equation displayed above\n",
        "\n",
        "def ChangeOfBasis(n): # n is the learned plane normal (what you get from the plane predictor net) (batch_size, 3, 1)\n",
        "    # reorientation of the coordinate system to align it with the dynamic plane\n",
        "    n = n.to(device)\n",
        "    # these unit vectors need a shape (batch_size*L, 3, 1)\n",
        "    i = torch.tensor([1.,0.,0.], device=device).repeat(n.shape[0], 1).unsqueeze(2)\n",
        "    j = torch.tensor([0.,1.,0.], device=device).repeat(n.shape[0], 1).unsqueeze(2)\n",
        "    k = torch.tensor([0.,0.,1.], device=device).repeat(n.shape[0], 1).unsqueeze(2)\n",
        "\n",
        "    n_hat = n/torch.norm(n)\n",
        "    v = skew_symm(k) @ n_hat# cross product btw k and n_hat\n",
        "    R = torch.eye(3).to(device).unsqueeze(0).repeat(n.shape[0], 1, 1) + skew_symm(v) + skew_symm(v).matmul(skew_symm(v))*(1 - dot_prod(k,n_hat))/(torch.norm(v)**2)\n",
        "    # this rotation matrix is applied to i and j\n",
        "\n",
        "    i_p = R @ i\n",
        "    j_p = R @ j\n",
        "\n",
        "    if torch.norm(i_p) != 1 : i_p = i_p/torch.norm(i_p)\n",
        "    if torch.norm(j_p) != 1 : j_p = j_p/torch.norm(j_p)\n",
        "\n",
        "    # return i_p, j_p\n",
        "    return i_p.reshape(BATCH_SIZE, NUM_PLANES, 3, 1), j_p.reshape(BATCH_SIZE, NUM_PLANES, 3, 1)\n",
        "\n",
        "def OrthographicProjection(p, i_p, j_p): # probably has more sense to simply use R\n",
        "    # p is the set of points that must be projected while i_p and j_p are the axes of the plane\n",
        "    if len(p.shape) < 4:\n",
        "        p = p.unsqueeze(1).repeat(1, i_p.shape[1], 1, 1) # (b,p,3) -> (b,L,p,3)\n",
        "        p = p.view(i_p.shape[0]*i_p.shape[1], p.shape[2], 3) # (b,L,p,3) -> (b*L,p,3)\n",
        "\n",
        "    if len(i_p.shape) > 3: # assume that both i_p and j_p have the same size\n",
        "        i_p = i_p.view(i_p.shape[0]*i_p.shape[1], i_p.shape[2], 1)\n",
        "        j_p = j_p.view(j_p.shape[0]*j_p.shape[1], j_p.shape[2], 1)\n",
        "\n",
        "    x_p = torch.bmm(p, i_p)\n",
        "    y_p = torch.bmm(p, j_p)\n",
        "\n",
        "    return torch.stack([x_p, y_p], dim=2).view(BATCH_SIZE, NUM_PLANES, p.shape[1], 2).to(device)\n",
        "\n",
        "def ComputeNormConst(i_p, j_p): # use this to compute directly the normalization constant c >= 1\n",
        "\n",
        "    i_p_abs = torch.abs(i_p.view(i_p.shape[0]*i_p.shape[1], i_p.shape[2], 1))\n",
        "    j_p_abs = torch.abs(j_p.view(j_p.shape[0]*j_p.shape[1], j_p.shape[2], 1))\n",
        "\n",
        "    ones = torch.ones((i_p_abs.shape[0], 1, 3)).to(device)\n",
        "\n",
        "    a_i = torch.bmm(ones, i_p_abs)/(torch.bmm(i_p_abs.transpose(2,1),i_p_abs)) * i_p_abs\n",
        "    a_j = torch.bmm(ones, j_p_abs)/(torch.bmm(j_p_abs.transpose(2,1),j_p_abs)) * j_p_abs\n",
        "\n",
        "    c = torch.max(torch.norm(a_i, dim=1), torch.norm(a_j, dim=1))\n",
        "    c = torch.clamp(c, min=1.0) # must be >= 1\n",
        "    return c # (b*L, 1, 1)\n",
        "\n",
        "# Move everything into a grid\n",
        "\n",
        "def FeatureProjection(input_cloud, plane_features, plane_param, H=64, W=64):\n",
        "    # plane_features, plane_parame are the output of the plane predictor net\n",
        "    i_p, j_p = ChangeOfBasis(plane_param.reshape(BATCH_SIZE*NUM_PLANES, 3).unsqueeze(2)) # both are (b,L,3,1)\n",
        "    projected_plane = OrthographicProjection(input_cloud, i_p, j_p) # (b,L,p,2)\n",
        "    c = ComputeNormConst(i_p, j_p) #(b*L,1)\n",
        "    # normalization\n",
        "    norm_plane = projected_plane.view(c.shape[0], projected_plane.shape[2], 2)/c.unsqueeze(1) # (b*L,p,2)\n",
        "\n",
        "    # now we need to perform pooling inside each bucket of a grid H*W\n",
        "    x_norm = (norm_plane[:,:,0]*W).long().clamp(0,W-1)\n",
        "    y_norm = (norm_plane[:,:,1]*H).long().clamp(0,H-1)\n",
        "    indices = x_norm + W*y_norm# .view(BATCH_SIZE, NUM_PLANES, input_cloud.shape[1])# (b,L,p)\n",
        "\n",
        "    out_scatter, _ = scatter_max(plane_features.reshape(BATCH_SIZE*NUM_PLANES, input_cloud.shape[1], -1), indices, dim=1, dim_size=H*W)\n",
        "\n",
        "    return out_scatter.view(BATCH_SIZE*NUM_PLANES, H, W, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fME3ek-b9WKh"
      },
      "source": [
        "## 2.5) Complete Encoder Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wEnfYEQZ9WKh"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim=64, out_dim=32, n_points=2048, n_blocks=5, num_planes=NUM_PLANES, num_fc=4):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.resnet_pointnet = ResNetPointNet(in_dim=in_dim, n_points=n_points, out_dim=out_dim, n_blocks=n_blocks).to(device)\n",
        "        self.plane_predictor = PlanePredictor(in_dim=out_dim, n_points=n_points, n_fc=num_fc, L=num_planes).to(device)\n",
        "        self.UNet = UNet(in_dim=out_dim, out_dim=out_dim, features_dim=in_dim, n_points=n_points).to(device)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat_resnet = self.resnet_pointnet(x)\n",
        "        feat_plane, plane_param = self.plane_predictor(x)\n",
        "        summed_up_feat = (feat_resnet + feat_plane).unsqueeze(1).repeat(1, NUM_PLANES, 1, 1) # (b,p,32) -> (b,L,p,32)\n",
        "        # here we need to perform features projection\n",
        "\n",
        "        feat_proj = FeatureProjection(x, summed_up_feat, plane_param) # returns the projection of the features and the projected and norm plane\n",
        "        out_unet = self.UNet(feat_proj.permute(0,-1,1,2)) # need (b*L,32,H,W) instead of (b*L,H,W,32)\n",
        "        return out_unet, plane_param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Xfwdl8S9WKi"
      },
      "source": [
        "# 3] Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "W3Cu6WzB9WKi"
      },
      "outputs": [],
      "source": [
        "# After the Unet, I have L dynamic planes each with feature map H*W of dimension 32 for each batch\n",
        "# Before using the real decoder network I need to follows these steps:\n",
        "# > we need to project any point of a cloud onto all dynamic planes\n",
        "# > use bilinear interpolation of the feature encoded at the four neighboring plane grids (smooth)\n",
        "# > concatenate the L features obtained\n",
        "# > obtain occupancy prediction from the decoder\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "        This class is used to define the Decoder Network. The architecture is composed by 5 ResNet blocks with hidden dimension 32 followed by a small\n",
        "        Fully Connected netowrk that returns the Occupancy prediction.\n",
        "        Given the features vector in input, we have to perform Bilinear Interpolation before the ResNet blocks.\n",
        "        Architecture design:\n",
        "\n",
        "            @ INPUT:\n",
        "                > Tensor of shape [features vector] (batch_size, num_points, features=32*L)\n",
        "\n",
        "\n",
        "            @ OUTPUT:\n",
        "                > Tensor of shape [occupancy prediction] (batch_size, n_points=2048, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim=32*NUM_PLANES, n_points=2048, n_blocks=5):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.res_blocks = nn.ModuleList(\n",
        "            [ResBlock(in_dim, n_points, in_dim, in_dim) for i in range(n_blocks)]\n",
        "        )\n",
        "        self.occupancy_pred = nn.Sequential(\n",
        "            nn.Linear(in_dim, in_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_dim//2, 1),\n",
        "            nn.Sigmoid(), # choose if you directly apply bce with logits or not\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        for res_block in self.res_blocks:\n",
        "            x = res_block(x) # check if the addition of the feature vector is correct\n",
        "\n",
        "        occupancy = self.occupancy_pred(x)\n",
        "        # print(f\"Decoder : ({x.shape}) --> ({occupancy.shape}) \\n\")\n",
        "\n",
        "        return occupancy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbexQquN9WKi"
      },
      "source": [
        "# 4] Metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sampleMesh(mesh, sampling_size=100000):\n",
        "    # assume that mesh is directly a trimesh obj\n",
        "    points, faces_idx = mesh.sample(sampling_size, return_index=True)\n",
        "    return points, mesh.face_normals[faces_idx] # both are (P, 3) --> need to be extended for batches\n",
        "\n",
        "def nearestNeighborDistance(pred_mesh, gt_mesh):\n",
        "    # gives me the nearest neighbor distance that I have on the gt_mesh from my pred_mesh\n",
        "    tree = cKDTree(gt_mesh)\n",
        "    distances, indices = tree.query(pred_mesh, k=1)\n",
        "    return distances, indices"
      ],
      "metadata": {
        "id": "Y_CDVTUriATX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh9zVxRg_PxY"
      },
      "source": [
        "<font color=\"orange\"> Chamster Distance: </font> $\n",
        "CD(A, B) = \\frac{1}{|A|} \\sum_{a \\in A} \\min_{b \\in B} \\|a - b\\|_2^2 + \\frac{1}{|B|} \\sum_{b \\in B} \\min_{a \\in A} \\|b - a\\|_2^2\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb7uqm2c_Wbf"
      },
      "source": [
        "<font color=\"orange\"> Volumetric IOU: </font> $ IoU(A', B') = \\frac{|A' \\cap B'|}{|A' \\cup B'|}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMwdO44rEyJl"
      },
      "outputs": [],
      "source": [
        "def VolumetricIOU(gt_mesh, pred_mesh, sampling_size=100000):\n",
        "    # in the paper they sample 100k points from the bounding boxes of the mesh and check their occupancy in both mesh\n",
        "    # then you compute the IOU with the binary array that you have obtained\n",
        "    b_min = np.minimum(pred_mesh.bounds[0], gt_mesh.bounds[0])\n",
        "    b_max = np.maximum(pred_mesh.bounds[1], gt_mesh.bounds[1])\n",
        "    x = np.random.uniform(b_min[0], b_max[0], sampling_size)\n",
        "    y = np.random.uniform(b_min[1], b_max[1], sampling_size)\n",
        "    z = np.random.uniform(b_min[2], b_max[2], sampling_size)\n",
        "    sampled_points = np.column_stack((x, y, z)) # need to sample from a bounding box that contains both clouds\n",
        "\n",
        "    points_gt = gt_mesh.contains(sampled_points)\n",
        "    points_pred = pred_mesh.contains(sampled_points) # assume to have watertight mesh but this may not be the case (TODO)\n",
        "\n",
        "    inter = np.sum(np.logical_and(points_gt, points_pred))\n",
        "    union = np.sum(np.logical_or(points_gt, points_pred))\n",
        "\n",
        "    return inter/union"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyvI3CLJJ3Re"
      },
      "source": [
        "<font color=\"orange\"> F-Score: </font> $2\\frac{Prec \\cdot Rec}{Prec + Rec}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQEisF9oIkWx"
      },
      "outputs": [],
      "source": [
        "def FScore(gt_mesh, pred_mesh, sampling_size=100000, threshold=0.01):\n",
        "    # Precision: percentage of points on the predicted mesh that lie within a certain distance to the ground truth\n",
        "    # Recall: how many points on the ground truth lie within a distance to the predicted mesh\n",
        "\n",
        "    points_pred, _ = sampleMesh(pred_mesh, sampling_size//2)\n",
        "    points_gt, _ = sampleMesh(gt_mesh,   sampling_size//2)\n",
        "    dist_pred2gt, _ = nearestNeighborDistance(points_pred, points_gt)\n",
        "    dist_gt2pred, _ = nearestNeighborDistance(points_gt, points_pred)\n",
        "\n",
        "    # \"hit\" if dist < threshold\n",
        "    positive_pred = (dist_pred2gt < threshold)\n",
        "    positive_gt = (dist_gt2pred < threshold)\n",
        "\n",
        "    prec = np.sum(positive_pred) / float(len(positive_pred) + 1e-10)\n",
        "    rec = np.sum(positive_gt) / float(len(positive_gt) + 1e-10)\n",
        "\n",
        "    return 2.0 * (prec * rec) / (prec + rec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cBjgw_UJ5_R"
      },
      "source": [
        "<font color=\"orange\"> Normal Consistency: </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVAClDUMIl0T"
      },
      "outputs": [],
      "source": [
        "# Normal Consistency -> it's used to compare the normal of my predicted mesh vs the normal of the gt mesh\n",
        "# I want this to be high\n",
        "\n",
        "# sampling from the predicted mesh and get their normals\n",
        "# find the nearest neighbor on the gt for each sampled point and get its normal\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def computeMetrics(gt_mesh, pred_mesh, sampling_size=100000, threshold=0.01):\n",
        "    # assume directly trimesh obj\n",
        "\n",
        "    iou = VolumetricIOU(gt_mesh, pred_mesh, sampling_size=sampling_size)\n",
        "    fscore = FScore(gt_mesh, pred_mesh, sampling_size=sampling_size, threshold=0.01)\n",
        "\n",
        "    # chamfer_distance()\n",
        "    chamf = 0\n",
        "\n",
        "    norm_consist = 0\n",
        "\n",
        "    return {\"iou\": iou, \"fscore\": fscore, \"chamf\": chamf, \"norm_consist\": norm_consist}"
      ],
      "metadata": {
        "id": "swpOv_mTmwzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIpxjGH19WKj"
      },
      "source": [
        "# 5] Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "2tQR065UJKnf"
      },
      "outputs": [],
      "source": [
        "def addNoise(cloud, std=0.05):\n",
        "    noisy_cloud = cloud + np.random.normal(0, std, cloud.shape)\n",
        "    return noisy_cloud.to(torch.float32).to(device)\n",
        "\n",
        "def Cloud2Mesh(cloud):\n",
        "    point_cloud = toPointCloud(cloud)\n",
        "    mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_ball_pivoting(point_cloud, o3d.utility.DoubleVector([0.005, 0.01, 0.02, 0.1, 0.5]))\n",
        "    # mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(point_cloud, depth=9)\n",
        "    # densities = np.asarray(densities)\n",
        "    # density_threshold = np.percentile(densities, 5)\n",
        "    # vertices_to_keep = densities > density_threshold\n",
        "    # mesh = mesh.select_by_index(np.where(vertices_to_keep)[0])\n",
        "    # mesh.remove_non_manifold_edges()\n",
        "    # mesh.remove_degenerate_triangles()\n",
        "    # mesh.remove_duplicated_triangles()\n",
        "    # mesh.remove_duplicated_vertices()\n",
        "    return mesh\n",
        "\n",
        "def getOccupancyLabels(sampled_cloud, reg_name, threshold=0.05):\n",
        "    # registration which will gives us the watertight mesh\n",
        "    # sampled_cloud is the point cloud obtained by uniform sampling in the space of the cloud\n",
        "    label_container = []\n",
        "    # mesh = o3d.io.read_triangle_mesh(reg_name)\n",
        "    for b in range(sampled_cloud.shape[0]):\n",
        "        mesh = trimesh.load_mesh(reg_name[b])\n",
        "\n",
        "        labels = np.zeros(sampled_cloud.shape[1], dtype=np.int32)\n",
        "        distances = mesh.nearest.signed_distance(sampled_cloud[b])\n",
        "        # outside = negative distance\n",
        "        # inside = positive distance\n",
        "        labels[distances >= 0] = 1\n",
        "        labels[distances > -threshold] = 1\n",
        "        label_container.append(labels.reshape(-1,1))\n",
        "\n",
        "    return torch.tensor(label_container, dtype=torch.float32).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "jBGZGh8yBHMp"
      },
      "outputs": [],
      "source": [
        "# I use as input a noisy point cloud (point cloud + sampled noise with zero mean and stdev 0.05)\n",
        "# Uniformly sampled cloud is used as the occupancy supervision aka my label -> use this to found the real occupancy value !\n",
        "# Training with BCE\n",
        "# Inference -> Multiresolution Isosurface Extraction used to construct meshes starting from the occupancy grid\n",
        "class CompleteArchitecture(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim=64, out_dim=32, n_points_encoder=3000, n_points_decoder=2048, n_blocks=5, num_planes=4, num_fc=4):\n",
        "        super(CompleteArchitecture, self).__init__()\n",
        "        self.encoder = Encoder(n_points=n_points_encoder) # 3000 points from a noisy cloud given in input to learn the features\n",
        "        self.decoder = Decoder(n_points=n_points_decoder) # 2048 points sampled uniformly (both inside and outside the cloud)\n",
        "\n",
        "    def forward(self, noisy_cloud, sampled_cloud):\n",
        "        out_enc, plane_param = self.encoder(noisy_cloud)\n",
        "        # need to perform the projection of the sampled_cloud into the dynamic planes\n",
        "        i_p, j_p = ChangeOfBasis(plane_param.reshape(BATCH_SIZE*NUM_PLANES, 3).unsqueeze(2))\n",
        "        proj_plane = OrthographicProjection(sampled_cloud, i_p, j_p) # (B,L,p,2)\n",
        "        c = ComputeNormConst(i_p, j_p) #(b*L,1)\n",
        "        norm_plane = proj_plane.view(c.shape[0], proj_plane.shape[2], 2)/c.unsqueeze(1) # (b*L,p,2)\n",
        "\n",
        "        norm_plane = 2.0 * norm_plane - 1.0 # from [0,1] to [-1,1]\n",
        "        #print(norm_plane.shape)\n",
        "\n",
        "        interpol_feat = F.grid_sample(out_enc, norm_plane.unsqueeze(1), mode='bilinear', align_corners=True)\n",
        "        #print(interpol_feat.shape)\n",
        "        interpol_feat = interpol_feat.squeeze(2).permute(0,2,1).view(BATCH_SIZE, NUM_PLANES, sampled_cloud.shape[1], interpol_feat.shape[1])\n",
        "        #print(interpol_feat.shape)\n",
        "        concat_feat = interpol_feat.reshape(BATCH_SIZE, -1, NUM_PLANES*interpol_feat.shape[-1])\n",
        "        occupancy = self.decoder(concat_feat)\n",
        "        return occupancy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "04_fhuVFXcYs"
      },
      "outputs": [],
      "source": [
        "completeModel = CompleteArchitecture(n_points_encoder=3000, n_points_decoder=2048)\n",
        "completeModel = completeModel.to(device)\n",
        "BCE = nn.BCELoss()\n",
        "for batch in train_loader:\n",
        "    scan, registration, sampled_cloud, scan_list, reg_list = batch\n",
        "\n",
        "    sampled_cloud = sampled_cloud.to(device) # (B, P, 3)\n",
        "    noisy_cloud = addNoise(scan)# that's my actual input to the net (B, P, 3)\n",
        "    # Plot2D(noisy_cloud[0].cpu().numpy())\n",
        "    # Plot2D(sampled_cloud[0].cpu().numpy())\n",
        "    occupancy_pred = completeModel(noisy_cloud, sampled_cloud) # (B, P, 1)\n",
        "\n",
        "    occupancy_labels = getOccupancyLabels(sampled_cloud[0].cpu().numpy(), reg_list[0])\n",
        "    # PlotOccupancyMesh(reg_list[0], sampled_cloud[0].cpu().numpy(), occupancy_labels)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "heghvvVw9WKj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "497a695d-094e-429a-87e1-86a128e8f335"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-f0fc1f15dfc2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompleteModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCompleteArchitecture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcompleteModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompleteModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mBCE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# later change this and use the version with logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompleteModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# currently using the same from the paper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-484a5944d07d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_dim, out_dim, n_points_encoder, n_points_decoder, n_blocks, num_planes, num_fc)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_points_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_points_decoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_blocks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_planes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_fc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCompleteArchitecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_points_encoder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 3000 points from a noisy cloud given in input to learn the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_points_decoder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 2048 points sampled uniformly (both inside and outside the cloud)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-ae54fae6cbf6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_dim, out_dim, n_points, n_blocks, num_planes, num_fc)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet_pointnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNetPointNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_blocks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplane_predictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlanePredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_fc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_planes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1324\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                     )\n\u001b[0;32m-> 1326\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1327\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "completeModel = CompleteArchitecture()\n",
        "completeModel = completeModel.to(device)\n",
        "BCE = nn.BCELoss() # later change this and use the version with logits\n",
        "optimizer = torch.optim.Adam(completeModel.parameters(), lr=1e-4) # currently using the same from the paper\n",
        "num_epochs = 25\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    completeModel.train()\n",
        "\n",
        "    train_loss = 0.0\n",
        "\n",
        "    train_loss_cnt = []\n",
        "    val_loss_cnt = []\n",
        "\n",
        "    # add other metrics container (IOU, chamfer, ...)\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        scan, registration, sampled_cloud, scan_list, reg_list = batch\n",
        "        sampled_cloud = sampled_cloud.to(device) # (B, P, 3)\n",
        "        noisy_cloud = addNoise(scan) # that's my actual input to the net (B, P, 3)\n",
        "        occupancy_pred = completeModel(noisy_cloud, sampled_cloud) # (B, P, 1)\n",
        "        occupancy_labels = getOccupancyLabels(sampled_cloud.cpu().numpy(), reg_list)\n",
        "        # PlotOccupancyMesh(reg_list[0], sampled_cloud[0].cpu().numpy(), occupancy_labels)\n",
        "\n",
        "        loss = BCE(occupancy_pred, occupancy_labels)\n",
        "        print(occupancy_pred.dtype, occupancy_labels.dtype)\n",
        "        torch.autograd.set_detect_anomaly(True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    train_loss_cnt.append(avg_train_loss)\n",
        "\n",
        "\n",
        "    completeModel.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            occupancy_supervision, _ = batch\n",
        "\n",
        "            sampled_cloud = occupancy_supervision.to(device) # later used as occupancy supervision\n",
        "            noisy_cloud = occupancy_supervision + torch.random.normal(0, 0.05, occupancy_supervision.shape).to(device) # that's my actual input\n",
        "\n",
        "            occupancy_pred = completeModel(noisy_cloud)\n",
        "            # real_occupancy = do something with the occupancy supervision\n",
        "\n",
        "            loss = BCE(occupancy_pred, real_occupancy)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            # perform the reconstruction\n",
        "            # meshes = Reconstruction(occupancy_pred, sampled_cloud)\n",
        "            # compute other metrics\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_loss_cnt.append(avg_val_loss)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "    # save the model if it's better than the last one\n",
        "\n",
        "    \"\"\"\n",
        "    torch.save(\n",
        "        {\n",
        "        'model_state_dict': completeModel.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        },\n",
        "        'model.pt'\n",
        "    )\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypdSm5bEsFIp"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_loss_cnt, label='Train')\n",
        "plt.plot(val_loss_cnt, label='Val')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.title('Training and Validation Losses')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('losses.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u0r2Gpj9WKj"
      },
      "source": [
        "# 6] Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMacvgWc2qzw"
      },
      "source": [
        "We first discretize the volumetric space at an initial resolution and evaluate the occupancy network fθ (p, x) for all p in this grid.\n",
        "We mark all grid points p as <font color=\"orange\"> occupied </font> for which fθ(p,x) is bigger or equal to some threshold τ. Next, we mark all voxels as <font color=\"olive\"> active </font> for which **at least\n",
        "two adjacent grid points** have differing occupancy predictions. These are the voxels which would intersect the mesh with the marching cubes algorithm\n",
        "at the current resolution. We subdivide **all active voxels into**<font color=\"orange\"> 8 </font>**subvoxels** and evaluate all new grid points which are introduced to the occupancy grid through\n",
        "this subdivision. We repeat these steps until the desired final resolution is reached.\n",
        "At this final resolution, we apply the Marching Cubes algorithm to extract an approximate isosurface :\n",
        "{p ∈ R3 | fθ(p,x) = τ}.\n",
        "Our algorithm converges to the correct mesh if the occupancy grid at the initial resolution contains points from every connected component of both the\n",
        "interior and the exterior of the mesh. It is hence important to take an initial resolution which is high enough to satisfy this condition.\n",
        "In practice, we found that an initial resolution of $32^3$ was sufficient in almost all cases.\n",
        "The initial mesh extracted by the Marching Cubes algorithm can be further refined. In a first step, we simplify the mesh using the\n",
        "Fast-Quadric-Mesh-Simplification algorithm. Finally, we refine the output mesh using first and second order (i.e., gradient) information.\n",
        "Towards this goal, we sample random points pk from each face of the output mesh and minimize the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y1hIAvXePXg"
      },
      "outputs": [],
      "source": [
        "# MISE:\n",
        "# discretize volumetric space at initial resolution and mark as occupied the grid point with occupancy > threshold\n",
        "# if at least 2 adjacent grid points have differing occupancy predictions (this will be the intersecting voxels)\n",
        "# subdivide 8 subvoxels and evaluate them as before (continue this until you have reached the correct resolution)\n",
        "# use marching cubes\n",
        "# simplify mesh with Fast Quadric Mesh Simplification Algorithm\n",
        "# refine output with 1st or 2nd order gradient information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cj7jg46jzWKy"
      },
      "outputs": [],
      "source": [
        "# discretize volumetric space at initial resolution and mark as occupied the grid point with occupancy > threshold\n",
        "def DiscretizeVolumetricSpace(cloud, resolution=32, padding=0.1):\n",
        "    min_bound = cloud.min(dim=1)[0].min(dim=1)[0].cpu().numpy() - padding\n",
        "    max_bound = cloud.max(dim=1)[0].max(dim=1)[0].cpu().numpy() + padding\n",
        "\n",
        "    coord = np.linspace(min_bound, max_bound, resolution)\n",
        "    grid = np.stack(np.meshgrid(coord[0], coord[1], coord[2], indexing='ij'), axis=-1)\n",
        "    return grid.reshape(-1, 3) # (resolution^3, 3)\n",
        "\n",
        "# use the model to check the occupancy of the grid points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrLSz0rE123h"
      },
      "outputs": [],
      "source": [
        "# if at least 2 adjacent grid points have differing occupancy predictions then the voxel is Active (this will be the intersecting voxels with the mesh)\n",
        "def getActiveVoxels(grid_occupancy):\n",
        "    active_voxel_indices = np.argwhere(\n",
        "        (np.diff(grid_occupancy, axis=0) != 0) |\n",
        "        (np.diff(grid_occupancy, axis=1) != 0) |\n",
        "        (np.diff(grid_occupancy, axis=2) != 0) )\n",
        "    return active_voxel_indices.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jn9JJ7iR3k9H"
      },
      "outputs": [],
      "source": [
        "vertices, faces, normals, values = trimesh.voxel.ops.marching_cubes(\n",
        "        occupancy_grid,\n",
        "        level=0.5,  # Iso-surface threshold\n",
        "        spacing=( (max_bound - min_bound) / (resolution - 1) )\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9_0FERiAhMW8",
        "GFBi0FgT9WKe",
        "Rh8ugKUp9WKf",
        "8abAwINI9WKf",
        "QHoALeYs9WKg",
        "3BEqvWJV9WKh",
        "fME3ek-b9WKh",
        "_Xfwdl8S9WKi",
        "FbexQquN9WKi"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}